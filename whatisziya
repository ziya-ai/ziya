Helpful coding assistant that explains what it is suggesting and why, and allows you to selectively apply changes once you understand and agree with them.
Comprehensive in-line techncial visualizations, encompassing more than [ ] diagram types, using a D3 plugin architecture that incorporates graphviz, mermaid, vegalite, drawio, joint, mathml/katex, and others. Each visualization plugin has a preprocessing layer that normalizes content from any supported LLM.
Support for multiple backend providers, including Amazon Bedrock and Google Gemini. Models may be changed mid-conversation for second opinions.
Content aware text highlighting covering [ ] unique languages
Display and application of Diffs and patches. The patch application system automatically compensates for thousands of inexact patterns provided by LLMs, allowing patches to be applied instantly. A regression suite of over 345 edge-case patches ensures patches will apply cleanly regardless of language or source LLM.
Native reading of common file formats including pdf, docx, xlsx, and dynamic loading context helpers for more obscure file types like network pcap files.
Support for standard MCPs, and built-in system and shell access MCPs. Browsing and in-browser installation and configuration of many MCPs through mcp-registries.
Per-MCP pluggable prettyprint / output management.
Context management, with constant display of number of consumed tokens, allowing users to selectively remove content that may no longer be relevant, and perhaps most importantly, to preload entire source files or repositories that will help models answer questions rapidly without having to search around for relevant context.
Persistent conversation history. Conversations will never 'reset' and lose their brains because context is overflowing.
Images can be attached to conversation streams for models to interpret.
Conversations can be forked to follow tangential paths or fixes.
Conversations can be organized by project.
Individual conversations or entire histories can be exported or imported.
Any item in conversational history may be edited, resubmitted, forked, truncated, or removed. 
In-line HTML mockups for discussion of interfaces.
Pluggable stencil library for image generation.

Ziya is an internally developed (but partially externally open-sourced) first class bedrock client It is an alternative to Cline or Kiro, and has been under heavy private development for the last 18 months. Ziya is a context-first locally running web-based client that has extensive (and we mean that) integrated visualization support, code patch application support. It can look at entire codebases and complex projects and quickly provide architectural diagrams, architectural direction, network analytics, code analytics. Code approach is explanatory, and patches (provided in highlighted git diff format with highlighting for hundreds of languages) can be one-click applied (very extensive work has gone into making sure code always applies correctly without corruption, through a multi-stage normalization and validation pipeline.) In-line rendering support for Graphviz, Drawio, VegaLite, Mermaid, HTML Mockup, Joint, and Katex/MathML also has gone through extensive validation so that LLM generated visualizations display correctly. Ziya always tells you what your context utilization is, and allows you to add or remove things selectively to your contexts, so it never loses its mind. Integrated MCP servers include a safe shell interface that operates in the local filesystem and with git in a read-only fashion, so it cannot lose your changes or make other inadvisable changes to your codebase. Conversations are persistent. Supports image uploading, can natively read many file formats including docs, pdfs, ppt, xlsx, and pcap. Tested with buildermcp and other internal tools, has a plugin library for post-mcp output fixups. Supports mcp-registries with automatic mcp installations, and of course tracks your per-mcp and per-tool token space utilization. Users can select between any available bedrock model (presently ~20 supported) and change models or model parameters mid-conversation. Context tracks local file modifications so the system knows if you have chosen to apply its suggestions, or have gone away and made a bunch of external changes since the last conversation round and naturally picks up conversation from current codebase status knowing how that provenance relates to the timeline of the conversation. So much more. It can be your primary AI client as it has been mine for the past 6 months.
- Please specify if you need help with scaling, productizing and promoting the tool.
I realize that we're at a crucial point where if we don't announce this and get awareness spread, we may miss the window for new clients. I have developed a really awesome tool that I think everyone should be using. I have not developed any documentation or demos, and I've just started pushing it through ASR for general use and acceptance by ASBX. It just makes me nuts seeing people use Cline and Kiro/Q cli for tasks that I know they could be so much more productive with Ziya as their client if they only knew about it. So I'm looking for any avenues to show it off now. Too bad I just heard about the D&S AI showcase, because this would've been a great entry. I want to get it promoted to be considered an equal client to its peers, especially for people doing system architecture work, or people who like to develop or keep their AI tools separate from their IDEs or who don't like using IDEs in general.
